{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "from vision.classification_model import ClassificationModel\n",
    "from vision.segmentation_model import SegmentationModel\n",
    "# from vision.vision_models import SegmentationModel, ClassificationModel\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "from keras import backend as K\n",
    "from backend_logging import get_logger\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = \"img_size\"\n",
    "BATCH_SIZE = \"batch_size\"\n",
    "OPTIMIZER = \"optimizer\"\n",
    "EPOCHS = \"epochs\"\n",
    "LOSS = \"loss\"\n",
    "METRICS = \"metrics\"\n",
    "LEARNING_RATE = \"learning_rate\"\n",
    "PATIENCE = \"patience\"\n",
    "CALLBACKS = \"callbacks\"\n",
    "PATH = \"path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISION_PATH = \"vision/crops/monkey/predict\"\n",
    "#TODO: modify to raise instead of returns?\n",
    "class VisionClient():\n",
    "    def __init__(self):\n",
    "        self.pipeline = None\n",
    "        self.path = VISION_PATH\n",
    "        self.sm = None\n",
    "        self.cm = None\n",
    "        self.k = None\n",
    "        self.d = None\n",
    "    \n",
    "    def connect(self):\n",
    "        self.sm = SegmentationModel()\n",
    "        self.sm.load_model()\n",
    "        get_logger(__name__).log(logging.INFO,\n",
    "                        f\"Segmentation model loaded\")\n",
    "        K.clear_session()\n",
    "        self.cm = ClassificationModel()\n",
    "        self.cm.load_model()\n",
    "        get_logger(__name__).log(logging.INFO,\n",
    "                            f\"Classification model loaded\")\n",
    "        \n",
    "    def init_camera_streams(self):\n",
    "        # Configure depth and color streams\n",
    "        self.pipeline = rs.pipeline()\n",
    "        config = rs.config()\n",
    "\n",
    "        config.enable_stream(rs.stream.depth)\n",
    "        config.enable_stream(rs.stream.color)\n",
    "\n",
    "        # Start streaming\n",
    "        self.pipeline.start(config)\n",
    "        sensor = self.pipeline.get_active_profile().get_device().query_sensors()[1]\n",
    "\n",
    "        # Set the exposure anytime during the operation\n",
    "        sensor.set_option(rs.option.exposure, 500.000)\n",
    "        #sensor.set_option(rs.option.laser_power, 180)\n",
    "        align_to = rs.stream.color\n",
    "        align = rs.align(align_to)\n",
    "        filters = [rs.spatial_filter(),rs.temporal_filter()]\n",
    "\n",
    "        return align, filters\n",
    "\n",
    "    def get_frames(self, pipeline, align, filters):\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        for filter in filters:\n",
    "            depth_frame = filter.process(depth_frame)\n",
    "        depth_frame = depth_frame.as_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        self.color_intrin = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "        self.k = np.array(((self.color_intrin.fx, 0, self.color_intrin.ppx),\n",
    "                (0,self.color_intrin.fy, self.color_intrin.ppy),\n",
    "                (0,0,1)))\n",
    "        self.d = np.array(self.color_intrin.coeffs)\n",
    "        return color_frame, depth_frame\n",
    "\n",
    "    def get_valid_neighbors(self,coords,limits,size):\n",
    "        neighbours = []\n",
    "        for i in range(-size,size+1):\n",
    "            for j in range(-size,size+1):\n",
    "                xn, yn = coords[0]+i, coords[1]+j\n",
    "                if (xn,yn) != coords:\n",
    "                    if 0 <= xn < limits[0] and 0 <= yn < limits[1]:\n",
    "                        neighbours.append((xn,yn))\n",
    "        return neighbours\n",
    "\n",
    "    def get_2d_points(self,mask_raw, limits, c1):\n",
    "        polygon = mask_raw.xy[0]\n",
    "        epsilon = 0.1 * cv2.arcLength(polygon, True)\n",
    "        box = cv2.approxPolyDP(polygon, epsilon, True)\n",
    "        if box.shape != (4,1,2):\n",
    "            return\n",
    "        box = box.reshape(4,2)\n",
    "        #rect = cv2.minAreaRect(polygon)\n",
    "        #box = cv2.boxPoints(rect)\n",
    "        sorted_box = box[np.argsort(box[:, 0])]\n",
    "        top_points = [tuple(sorted_box[0]),tuple(sorted_box[1])]\n",
    "        c1.polygon(box,outline=(0,255,0),width=5)\n",
    "        c1.line(top_points,fill=(255,255,0),width=5)\n",
    "\n",
    "        top_x, top_y = np.mean(top_points, axis=0).astype(int)\n",
    "        center_x, center_y = np.mean(box,axis=0).astype(int)\n",
    "        \n",
    "        rel_2d_points = list(sorted_box.copy())\n",
    "        rel_2d_points.append([top_x,top_y])\n",
    "        rel_2d_points.append([center_x,center_y])\n",
    "\n",
    "        r = 5\n",
    "        for i, (x,y) in enumerate(rel_2d_points):\n",
    "            x = int(np.clip(x, 0, limits[0]-1))\n",
    "            y = int(np.clip(y, 0, limits[1]-1))\n",
    "            rel_2d_points[i] = [x,y]\n",
    "            c1.ellipse([(x-r,y-r),(x+r,y+r)],fill=(0,0,255))\n",
    "            \n",
    "        return rel_2d_points\n",
    "\n",
    "    def get_camera_3d_points(self,depth_frame, rel_2d_points, color_intrin, limits):\n",
    "        rel_3d_points = []\n",
    "        for x_2d,y_2d in rel_2d_points:\n",
    "            depth = depth_frame.get_distance(x_2d,y_2d)\n",
    "            if depth == 0:\n",
    "                neighbours = self.get_valid_neighbors((x_2d,y_2d),(limits[0],limits[1]),1)\n",
    "                distances = []\n",
    "                for nx, ny in neighbours:\n",
    "                    depth = depth_frame.get_distance(nx, ny)\n",
    "                    if depth != 0:\n",
    "                        distances.append(depth)\n",
    "                \n",
    "                if distances:\n",
    "                    depth = np.median(distances)\n",
    "                else:\n",
    "                    depth = 0\n",
    "                    print(\"No depths from self/neighbours could be found\")\n",
    "\n",
    "            #right: x, down: y, forward: z\n",
    "            result = rs.rs2_deproject_pixel_to_point(color_intrin, [x_2d, y_2d], depth)\n",
    "            \"\"\"\n",
    "            Camera:     Robot:\n",
    "            x: down     x: left\n",
    "            y: left     y: back\n",
    "            z: forward  z: up\n",
    "\n",
    "            Robot = Camera:\n",
    "            x -> y\n",
    "            y -> -z\n",
    "            z -> -x\n",
    "            \"\"\"\n",
    "            x_cam,y_cam,z_cam = result\n",
    "            x_rob = y_cam\n",
    "            y_rob = -z_cam\n",
    "            z_rob = -x_cam\n",
    "\n",
    "            rel_3d_points.append([x_rob,y_rob,z_rob])\n",
    "        return rel_3d_points\n",
    "\n",
    "    def show_3d_points(self,points_3d, rot, trans, k, d, c1):\n",
    "        for point_3d in points_3d:\n",
    "            point_3d = np.array(point_3d,dtype=np.float64)\n",
    "            point_2d, jacobian = cv2.projectPoints(point_3d, rot, trans, k, d)\n",
    "            x,y = tuple(point_2d.flatten())\n",
    "            r=5\n",
    "            c1.ellipse([(x-r,y-r),(x+r,y+r)],fill=(0,255,255))\n",
    "            #c1.text((x,y),f\"{point_3d}\")\n",
    "\n",
    "    def calculate_rotational_angles(self,plane_coordinates): #TODO: maybe modify\n",
    "        # Extracting the coordinates of the plane\n",
    "        p1, p2, p3, p4 = plane_coordinates\n",
    "\n",
    "        # Calculate vectors along two edges of the plane\n",
    "        v1 = np.array(p2) - np.array(p1)\n",
    "        v2 = np.array(p3) - np.array(p1)\n",
    "\n",
    "        # Calculate the cross product to get the normal vector of the plane\n",
    "        normal_vector = np.cross(v1, v2)\n",
    "        # Normalize the normal vector\n",
    "        normal_vector /= np.linalg.norm(normal_vector)\n",
    "\n",
    "        # Calculate angles around x, y, and z axes (Euler angles)\n",
    "        x_angle = np.arctan2(normal_vector[2], normal_vector[1])\n",
    "        y_angle = np.arctan2(-normal_vector[0], np.sqrt(normal_vector[1]**2 + normal_vector[2]**2))\n",
    "        z_angle = np.arctan2(v2[0], v1[0])\n",
    "\n",
    "        # Convert angles from radians to degrees\n",
    "        x_angle_deg = np.degrees(x_angle)\n",
    "        y_angle_deg = np.degrees(y_angle)\n",
    "        z_angle_deg = np.degrees(z_angle)\n",
    "\n",
    "        return x_angle_deg, y_angle_deg, z_angle_deg\n",
    "\n",
    "    def get_robot_coords(self, camera_coords):\n",
    "        R = np.array([[0.98965,0.14059,-0.028841],\n",
    "                    [-0.14345,0.97505,-0.16939],\n",
    "                    [0.0043076,0.17177,0.98513]])\n",
    "        t = np.array([0.18212,0.11633,0.38649])\n",
    "        new_coords = np.dot(R,camera_coords) + t\n",
    "\n",
    "        return new_coords\n",
    "\n",
    "    def get_pickup_locations(self):\n",
    "        d3_coords = []\n",
    "        align, filters = self.init_camera_streams()\n",
    "        try:\n",
    "            while True:\n",
    "                color_frame, depth_frame = self.get_frames(self.pipeline, align, filters)\n",
    "                if not depth_frame or not color_frame:\n",
    "                    continue\n",
    "            \n",
    "                color_image = np.asanyarray(color_frame.get_data())\n",
    "                img_color = Image.fromarray(color_image)\n",
    "                img_color.save(\"vision/input_color.jpg\")\n",
    "\n",
    "                results = self.sm.predict(color_image,False,True,True)\n",
    "                classes, conf_list = self.cm.predict(self.path+\"/crops/Crate/\")\n",
    "                data_image = Image.open(self.path+\"/image0.jpg\") #seg prediction results\n",
    "                color_draw = ImageDraw.Draw(data_image)\n",
    "                depth_image = np.asanyarray(depth_frame.get_data())\n",
    "                min_depth = 850  # Minimum depth value\n",
    "                max_depth = 1700  # Maximum depth value\n",
    "                depth_image_clipped = np.clip(depth_image, min_depth, max_depth)\n",
    "                normalized_depth = (depth_image_clipped - min_depth) / (max_depth - min_depth)\n",
    "                depth_colormap = cv2.applyColorMap((normalized_depth * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "                cv2.imwrite(\"vision/depth.jpg\", depth_colormap)\n",
    "\n",
    "                masks = results[0].masks\n",
    "                #for mask_raw, label in zip(masks,classes): #do classification result interpretation\n",
    "                for (mask_raw,cls) in zip(masks,classes):        \n",
    "                    rel_2d_points = self.get_2d_points(mask_raw, data_image.size, color_draw)\n",
    "                    if not rel_2d_points:\n",
    "                        print(\"Box has incorrect shape -> no crate\")\n",
    "                        continue\n",
    "\n",
    "                    rel_3d_points = self.get_camera_3d_points(depth_frame, rel_2d_points, self.color_intrin,data_image.size)\n",
    "                    success, rot, trans = cv2.solvePnP(np.array(rel_3d_points).astype(\"float32\"),np.array(rel_2d_points).astype(\"float32\"),self.k,self.d)\n",
    "                    self.show_3d_points(rel_3d_points[:4],rot, trans, self.k, self.d, color_draw)\n",
    "\n",
    "                    robot_coords = []\n",
    "                    for coord in rel_3d_points[:4]:\n",
    "                        rob_coord = self.get_robot_coords(coord)\n",
    "                        robot_coords.append(rob_coord)\n",
    "            \n",
    "                    x,y,z = self.get_robot_coords(rel_3d_points[4])\n",
    "                    rx,ry,rz = self.calculate_rotational_angles(robot_coords)\n",
    "                    d3_coords.append({\"coords\":(x,y,z,rx,ry,rz),\"class\":cls})\n",
    "                    point = tuple(round(c,5) for c in (x,y,z,rx,ry,rz))\n",
    "                    print(\"Robot_point:\",point)\n",
    "                    color_draw.text(rel_2d_points[4], f\"{cls,x,y,z,rx,ry,rz},{cls}\", fill=(255,255,255))\n",
    "                    \n",
    "                #data_image.show()\n",
    "                data_image.save(\"vision/distance_annot.jpg\")\n",
    "                files = glob.glob(os.path.join(self.path, '**/*.jpg'), recursive=True) #TODO: move outside of vision function\n",
    "                for f in files:\n",
    "                    os.remove(f)\n",
    "                break\n",
    "            return d3_coords\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        finally:\n",
    "            self.pipeline.stop()\n",
    "    \n",
    "    def get_valid_pickup_loc(self):\n",
    "        results = self.get_pickup_locations()\n",
    "        \"\"\"\n",
    "        Goal:\n",
    "        - Remove invalid results (limits & NoCrate)\n",
    "        - Search for highest crate\n",
    "        - If PickupCrate - return coords\n",
    "        - If NoPickupCrate - return assistance message\n",
    "        \"\"\"\n",
    "        val_results = []\n",
    "        for res in results:\n",
    "            coords = res[\"coords\"]\n",
    "            cls = res[\"class\"]\n",
    "            if not -1.5 < coords[0] < 1.5 or not -1.6 < coords[1] < -0.6 or coords[2] > 1:\n",
    "                print(\"Coords not within limits!, Result ignored\")\n",
    "                continue\n",
    "            elif cls == \"NoCrate\":\n",
    "                print(\"Ignored due to NoCrate class\")\n",
    "            else:\n",
    "                val_results.append(res)\n",
    "        \n",
    "        if val_results:\n",
    "            def get_coord_2(item):\n",
    "                return item[\"coords\"][2]\n",
    "            highest_entry = max(val_results, key=get_coord_2)\n",
    "\n",
    "            if highest_entry[\"class\"] == \"NoPickupCrate\":\n",
    "                return \"NoPickupCrate\",highest_entry[\"class\"]\n",
    "            else:\n",
    "                self.get_crate_height(highest_entry[\"coords\"]) #TODO\n",
    "                return highest_entry[\"coords\"]\n",
    "        else:\n",
    "            return \"No valid results\"\n",
    "    \n",
    "    def get_crate_height(self,coords):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = VisionClient()\n",
    "vision.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 Crates, 194.6ms\n",
      "Speed: 1.9ms preprocess, 194.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mvision\\crops\\monkey\\predict\u001b[0m\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Robot_point: (-0.15157, -0.89374, -0.34829, 175.53895, 0.72644, -134.2527)\n",
      "Robot_point: (-0.17534, -1.01843, -0.01109, -8.80438, 1.08913, -5.73659)\n",
      "Robot_point: (0.19964, -1.00784, -0.31404, 6.78037, 3.43819, 2.81959)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.1753387231684625,\n",
       " -1.0184257440808415,\n",
       " -0.011093732099688058,\n",
       " -8.80437993469011,\n",
       " 1.0891265710238327,\n",
       " -5.736585820101864)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision.get_valid_pickup_loc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Model\n",
    "\n",
    "This model is responsible for extracting crate front segments out of the picture of the entire carrier. These crops are then saved and forwarded to the classification model, as well as used for the pickup-point calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_model = SegmentationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_params = {\n",
    "    PATH: \"./vision/data/no-bars_no-classes_v2/data.yaml\",\n",
    "    IMG_SIZE: 640,\n",
    "    OPTIMIZER: \"AdamW\",\n",
    "    LEARNING_RATE: 0.0015,\n",
    "    EPOCHS: 80,\n",
    "    BATCH_SIZE: 8,\n",
    "    PATIENCE: 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load previously trained model\n",
    "segment_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.227 üöÄ Python-3.10.11 torch-2.1.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=segment, mode=train, model=vision/yolov8s-seg.pt, data=./vision/data/no-bars_no-classes/data.yaml/data.yaml, epochs=80, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train20, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=True, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.0015, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\segment\\train20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'vision/data/no-bars_no-classes/data.yaml/data.yaml' error  './vision/data/no-bars_no-classes/data.yaml/data.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\ultralytics\\engine\\trainer.py:116\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myml\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n",
      "File \u001b[1;32mc:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\ultralytics\\data\\utils.py:253\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[1;34m(dataset, autodownload)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03mDownload, verify, and/or unzip a dataset if not found locally.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    (dict): Parsed dataset information and paths.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Download (optional)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\ultralytics\\utils\\checks.py:460\u001b[0m, in \u001b[0;36mcheck_file\u001b[1;34m(file, suffix, download, hard)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hard:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: './vision/data/no-bars_no-classes/data.yaml/data.yaml' does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msegment_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseg_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\vision\\segmentation_model.py:35\u001b[0m, in \u001b[0;36mSegmentationModel.train\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_model()\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mlr0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mretina_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\ultralytics\\engine\\model.py:333\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    331\u001b[0m     args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[1;32mc:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\ultralytics\\models\\yolo\\segment\\train.py:30\u001b[0m, in \u001b[0;36mSegmentationTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m     28\u001b[0m     overrides \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     29\u001b[0m overrides[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\ultralytics\\engine\\trainer.py:120\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m error ‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset 'vision/data/no-bars_no-classes/data.yaml/data.yaml' error  './vision/data/no-bars_no-classes/data.yaml/data.yaml' does not exist"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "segment_model.train(seg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "segment_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try model out on image(s)\n",
    "results = segment_model.predict(seg_params[PATH]+\"/test/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "This model is used to take cropped images of crate fronts and classify them by their pickup method. These classifications will then be combined to determine pickup point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_model = ClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load previous model\n",
    "class_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 328ms/step\n",
      "Predicted class: NoCrate, 85.59 confidence.\n",
      "Real class: NoCrate\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted class: NoCrate, 91.35 confidence.\n",
      "Real class: NoCrate\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted class: NoPickupCrate, 99.91 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted class: NoPickupCrate, 85.45 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 90.86 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted class: NoPickupCrate, 99.74 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 99.97 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 99.95 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 99.89 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 99.99 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 99.91 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoCrate, 63.65 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted class: NoPickupCrate, 100.00 confidence.\n",
      "Real class: PickupCrate\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: PickupCrate, 100.00 confidence.\n",
      "Real class: NoPickupCrate\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#evaluate model\n",
    "class_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SegmentationModel()\n",
    "sm.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\simon\\OneDrive\\Schule\\2023_WS\\THUAS\\colruyt\\colruyt-destack\\.venv\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = ClassificationModel()\n",
    "cm.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_camera_streams():\n",
    "    # Configure depth and color streams\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "\n",
    "    config.enable_stream(rs.stream.depth)\n",
    "    config.enable_stream(rs.stream.color)\n",
    "\n",
    "    # Start streaming\n",
    "    pipeline.start(config)\n",
    "    sensor = pipeline.get_active_profile().get_device().query_sensors()[1]\n",
    "\n",
    "    # Set the exposure anytime during the operation\n",
    "    sensor.set_option(rs.option.exposure, 500.000)\n",
    "    #sensor.set_option(rs.option.laser_power, 180)\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "\n",
    "    font = ImageFont.truetype(\"vision/data/Arial.ttf\", 40)\n",
    "    filters = [rs.spatial_filter(),rs.temporal_filter()]\n",
    "    return pipeline, align, filters, font\n",
    "\n",
    "def get_frames(pipeline, align, filters):\n",
    "    # Wait for a coherent pair of frames: depth and color\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    aligned_frames = align.process(frames)\n",
    "    depth_frame = aligned_frames.get_depth_frame()\n",
    "    for filter in filters:\n",
    "        depth_frame = filter.process(depth_frame)\n",
    "    depth_frame = depth_frame.as_depth_frame()\n",
    "    color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "    color_intrin = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "    k = np.array(((color_intrin.fx, 0, color_intrin.ppx),\n",
    "            (0,color_intrin.fy, color_intrin.ppy),\n",
    "            (0,0,1)))\n",
    "    d = np.array(color_intrin.coeffs)\n",
    "    return color_frame, depth_frame, k, d, color_intrin\n",
    "\n",
    "def get_valid_neighbors(coords,limits,size):\n",
    "    neighbours = []\n",
    "    for i in range(-size,size+1):\n",
    "        for j in range(-size,size+1):\n",
    "            xn, yn = coords[0]+i, coords[1]+j\n",
    "            if (xn,yn) != coords:\n",
    "                if 0 <= xn < limits[0] and 0 <= yn < limits[1]:\n",
    "                    neighbours.append((xn,yn))\n",
    "    return neighbours\n",
    "\n",
    "def get_2d_points(mask_raw, limits, c1):\n",
    "    polygon = mask_raw.xy[0]\n",
    "    epsilon = 0.1 * cv2.arcLength(polygon, True)\n",
    "    box = cv2.approxPolyDP(polygon, epsilon, True)\n",
    "    if box.shape != (4,1,2):\n",
    "        return\n",
    "    box = box.reshape(4,2)\n",
    "    #rect = cv2.minAreaRect(polygon)\n",
    "    #box = cv2.boxPoints(rect)\n",
    "    sorted_box = box[np.argsort(box[:, 0])]\n",
    "    top_points = [tuple(sorted_box[0]),tuple(sorted_box[1])]\n",
    "    c1.polygon(box,outline=(0,255,0),width=5)\n",
    "    c1.line(top_points,fill=(255,255,0),width=5)\n",
    "\n",
    "    top_x, top_y = np.mean(top_points, axis=0).astype(int)\n",
    "    center_x, center_y = np.mean(box,axis=0).astype(int)\n",
    "    \n",
    "    rel_2d_points = list(sorted_box.copy())\n",
    "    rel_2d_points.append([top_x,top_y])\n",
    "    rel_2d_points.append([center_x,center_y])\n",
    "\n",
    "    r = 5\n",
    "    for i, (x,y) in enumerate(rel_2d_points):\n",
    "        x = int(np.clip(x, 0, limits[0]-1))\n",
    "        y = int(np.clip(y, 0, limits[1]-1))\n",
    "        rel_2d_points[i] = [x,y]\n",
    "        c1.ellipse([(x-r,y-r),(x+r,y+r)],fill=(0,0,255))\n",
    "        \n",
    "    return rel_2d_points\n",
    "\n",
    "def get_camera_3d_points(depth_frame, rel_2d_points, color_intrin, limits):\n",
    "    rel_3d_points = []\n",
    "    for x_2d,y_2d in rel_2d_points:\n",
    "        depth = depth_frame.get_distance(x_2d,y_2d)\n",
    "        if depth == 0:\n",
    "            neighbours = get_valid_neighbors((x_2d,y_2d),(limits[0],limits[1]),1)\n",
    "            distances = []\n",
    "            for nx, ny in neighbours:\n",
    "                depth = depth_frame.get_distance(nx, ny)\n",
    "                if depth != 0:\n",
    "                    distances.append(depth)\n",
    "            \n",
    "            if distances:\n",
    "                depth = np.median(distances)\n",
    "            else:\n",
    "                depth = 0\n",
    "                print(\"No depths from self/neighbours could be found\")\n",
    "\n",
    "        #right: x, down: y, forward: z\n",
    "        result = rs.rs2_deproject_pixel_to_point(color_intrin, [x_2d, y_2d], depth)\n",
    "        \"\"\"\n",
    "        Camera:     Robot:\n",
    "        x: down     x: left\n",
    "        y: left     y: back\n",
    "        z: forward  z: up\n",
    "\n",
    "        Robot = Camera:\n",
    "        x -> y\n",
    "        y -> -z\n",
    "        z -> -x\n",
    "        \"\"\"\n",
    "        x_cam,y_cam,z_cam = result\n",
    "        x_rob = y_cam\n",
    "        y_rob = -z_cam\n",
    "        z_rob = -x_cam\n",
    "\n",
    "        rel_3d_points.append([x_rob,y_rob,z_rob])\n",
    "    return rel_3d_points\n",
    "\n",
    "def draw_angle_lines(rel_3d_points,rot,trans,k,d,c1):\n",
    "    line1_points = [tuple(rel_3d_points[0]),tuple(rel_3d_points[1])]\n",
    "    line2_points = [tuple(rel_3d_points[0]),tuple(rel_3d_points[2])]\n",
    "    line1_vec = np.array(line1_points[1]) - np.array(line1_points[0])\n",
    "    line2_vec = np.array(line2_points[1]) - np.array(line2_points[0])\n",
    "    normal_vector = np.cross(line1_vec, line2_vec)\n",
    "    normal_vector_unit = normal_vector / np.linalg.norm(normal_vector)\n",
    "    point_on_vector = normal_vector_unit * 0.1\n",
    "    line3_points = [tuple(rel_3d_points[0]),tuple(point_on_vector)]\n",
    "    points = line1_points + line2_points + line3_points\n",
    "\n",
    "    displayed_points = []\n",
    "    for point in points:\n",
    "        point_2d, jacobian = cv2.projectPoints(np.array(point), rot, trans, k, d)\n",
    "        displayed_points.append(tuple(point_2d.flatten()))\n",
    "    \n",
    "    tuples_list = [(displayed_points[i], displayed_points[i + 1]) for i in range(0, len(displayed_points) - 1, 2)]\n",
    "    \n",
    "    for points in tuples_list:\n",
    "        c1.line(points,fill=(255,255,255),width=5)\n",
    "\n",
    "def show_3d_points(points_3d, rot, trans, k, d, c1):\n",
    "    for point_3d in points_3d:\n",
    "        point_3d = np.array(point_3d,dtype=np.float64)\n",
    "        point_2d, jacobian = cv2.projectPoints(point_3d, rot, trans, k, d)\n",
    "        x,y = tuple(point_2d.flatten())\n",
    "        r=5\n",
    "        c1.ellipse([(x-r,y-r),(x+r,y+r)],fill=(0,255,255))\n",
    "        #print(point_3d)\n",
    "        #c1.text((x,y),f\"{point_3d}\")\n",
    "\n",
    "def calculate_rotational_angles(plane_coordinates):\n",
    "    # Extracting the coordinates of the plane\n",
    "    p1, p2, p3, p4 = plane_coordinates\n",
    "\n",
    "    # Calculate vectors along two edges of the plane\n",
    "    v1 = np.array(p2) - np.array(p1)\n",
    "    v2 = np.array(p3) - np.array(p1)\n",
    "\n",
    "    # Calculate the cross product to get the normal vector of the plane\n",
    "    normal_vector = np.cross(v1, v2)\n",
    "    # Normalize the normal vector\n",
    "    normal_vector /= np.linalg.norm(normal_vector)\n",
    "\n",
    "    # Calculate angles around x, y, and z axes (Euler angles)\n",
    "    x_angle = np.arctan2(normal_vector[2], normal_vector[1])\n",
    "    y_angle = np.arctan2(-normal_vector[0], np.sqrt(normal_vector[1]**2 + normal_vector[2]**2))\n",
    "    z_angle = np.arctan2(v2[0], v1[0])\n",
    "\n",
    "    # Convert angles from radians to degrees\n",
    "    x_angle_deg = np.degrees(x_angle)\n",
    "    y_angle_deg = np.degrees(y_angle)\n",
    "    z_angle_deg = np.degrees(z_angle)\n",
    "\n",
    "    return x_angle_deg, y_angle_deg, z_angle_deg\n",
    "\n",
    "def get_robot_coords(camera_coords):\n",
    "    R = np.array([[0.98965,0.14059,-0.028841],\n",
    "                [-0.14345,0.97505,-0.16939],\n",
    "                [0.0043076,0.17177,0.98513]])\n",
    "    t = np.array([0.18212,0.11633,0.38649])\n",
    "    new_coords = np.dot(R,camera_coords) + t\n",
    "\n",
    "    return new_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 Crates, 176.7ms\n",
      "Speed: 8.0ms preprocess, 176.7ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mvision\\crops\\monkey\\predict\u001b[0m\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Box has incorrect shape -> no crate\n"
     ]
    }
   ],
   "source": [
    "pipeline, align, filters, font = init_camera_streams()\n",
    "path = \"vision/crops/monkey/predict\"\n",
    "try:\n",
    "    while True:\n",
    "        color_frame, depth_frame, k, d, color_intrin = get_frames(pipeline, align, filters)\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "    \n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        img_color = Image.fromarray(color_image)\n",
    "        #img_color.save(\"input_color.jpg\")\n",
    "\n",
    "        results = sm.predict(color_image,False,True,True)\n",
    "        classes, conf_list = cm.predict(path+\"/crops/Crate/\")\n",
    "        data_image = Image.open(path+\"/image0.jpg\") #seg prediction results\n",
    "        color_draw = ImageDraw.Draw(data_image)\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        min_depth = 850  # Minimum depth value\n",
    "        max_depth = 1700  # Maximum depth value\n",
    "        depth_image_clipped = np.clip(depth_image, min_depth, max_depth)\n",
    "        normalized_depth = (depth_image_clipped - min_depth) / (max_depth - min_depth)\n",
    "        depth_colormap = cv2.applyColorMap((normalized_depth * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        cv2.imwrite(\"depth.jpg\", depth_colormap)\n",
    "\n",
    "        masks = results[0].masks\n",
    "        #for mask_raw, label in zip(masks,classes): #do classification result interpretation\n",
    "        for (mask_raw,cls) in zip(masks,classes):        \n",
    "            rel_2d_points = get_2d_points(mask_raw, data_image.size, color_draw)\n",
    "            if not rel_2d_points:\n",
    "                print(\"Box has incorrect shape -> no crate\")\n",
    "                continue\n",
    "\n",
    "            rel_3d_points = get_camera_3d_points(depth_frame, rel_2d_points, color_intrin,data_image.size)\n",
    "            success, rot, trans = cv2.solvePnP(np.array(rel_3d_points).astype(\"float32\"),np.array(rel_2d_points).astype(\"float32\"),k,d)\n",
    "            show_3d_points(rel_3d_points[:4],rot, trans, k, d, color_draw)\n",
    "            #draw_angle_lines(rel_3d_points,rot,trans,k,d,color_draw)\n",
    "\n",
    "            robot_coords = []\n",
    "            for coord in rel_3d_points[:4]:\n",
    "                rob_coord = get_robot_coords(coord)\n",
    "                robot_coords.append(rob_coord)\n",
    "            \n",
    "            x,y,z = get_robot_coords(rel_3d_points[4])\n",
    "            rx,ry,rz = calculate_rotational_angles(robot_coords)\n",
    "            point = tuple(round(c,5) for c in (x,y,z,rx,ry,rz))\n",
    "            #print(point)\n",
    "            color_draw.text(rel_2d_points[4], f\"{cls,x,y,z,rx,ry,rz}\", fill=(255,255,255), font=font)\n",
    "            \n",
    "        data_image.show()\n",
    "        data_image.save(\"distance_annot.jpg\")\n",
    "        files = glob.glob(os.path.join(path, '**/*.jpg'), recursive=True)\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "finally:\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_camera = np.array([[   -0.39448,     -1.221,    -0.55261], #r-tr\n",
    "[   -0.10704,      -1.157,    -0.53379], #r-tl\n",
    "[    -0.4029,      -1.273,    -0.77989], #r-br\n",
    "[    -0.1082,      -1.198,    -0.75496], #r-bl\n",
    "[  -0.042201,       -1.27,    -0.50796], #l-tr\n",
    "[    0.33938,      -1.181,    -0.50343], #l-tl\n",
    "[    -0.0349,       -1.31,     -0.7566], #l-br\n",
    "[    0.34199,      -1.218,    -0.74352]]) #l-bl\n",
    "\n",
    "points_robot = np.array([[-72.41,-919.07,-354.64],#r-tr\n",
    "                [-363.73,-919.81,-357.03],#r-tl\n",
    "                [-71.73,-918.96,-579.00],#r-br\n",
    "                [-359.95,-921.61,-581.44],#r-bl\n",
    "                [-23.41,-1017.25,-326.41],#l-tr\n",
    "                [367.14,-1003.96,-323.79],#l-tl\n",
    "                [-23.66,-1019.09,-567.59],#l-br\n",
    "                [366.95,-1004.40,-564.53]])#l-bl\n",
    "points_robot = np.divide(points_robot,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation matrix:\n",
      "[[    0.98965     0.14059   -0.028841]\n",
      " [   -0.14345     0.97505    -0.16939]\n",
      " [  0.0043076     0.17177     0.98513]]\n",
      "\n",
      "Translation vector:\n",
      "[    0.18212     0.11633     0.38649]\n",
      "New coords: [   -0.36399    -0.92401    -0.36933]\n"
     ]
    }
   ],
   "source": [
    "centroid_camera = np.mean(points_camera, axis=0)\n",
    "centroid_robot = np.mean(points_robot, axis=0)\n",
    "\n",
    "centered_points_camera = points_camera - centroid_camera\n",
    "centered_points_robot = points_robot - centroid_robot\n",
    "\n",
    "# Singular Value Decomposition\n",
    "H = np.dot(centered_points_camera.T, centered_points_robot)\n",
    "U, S, Vt = np.linalg.svd(H)\n",
    "\n",
    "# Calculate rotation matrix\n",
    "R = np.dot(Vt.T, U.T)\n",
    "\n",
    "# Handle reflection case\n",
    "if np.linalg.det(R) < 0:\n",
    "    Vt[2, :] *= -1\n",
    "    R = np.dot(Vt.T, U.T)\n",
    "\n",
    "# Calculate translation\n",
    "t = centroid_robot - np.dot(R, centroid_camera)\n",
    "\n",
    "print(\"Rotation matrix:\")\n",
    "print(R)\n",
    "print(\"\\nTranslation vector:\")\n",
    "print(t)\n",
    "old_coords = np.array(points_camera[0])\n",
    "new_coords = np.dot(R,old_coords) + t\n",
    "print(\"New coords:\",new_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([[0.98965,0.14059,-0.028841],\n",
    "              [-0.14345,0.97505,-0.16939],\n",
    "              [0.0043076,0.17177,0.98513]])\n",
    "t = np.array([0.18212,0.11633,0.38649])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Pipeline\n",
    "- Take Picture\n",
    "- Get Segmentation Results\n",
    "- (Get Classification Results)\n",
    "- Augment "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
